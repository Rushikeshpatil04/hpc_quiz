<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>M5: Hadoop Administration - 200 Q Bank</title>
    <style>
        :root { --hadoop-yellow: #ffc107; --hadoop-blue: #0275d8; --hadoop-dark: #212529; --bg: #f8f9fa; --success: #28a745; --wrong: #dc3545; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: var(--bg); margin: 0; padding: 20px; }
        .quiz-container { max-width: 1100px; margin: auto; background: white; padding: 30px; border-radius: 15px; box-shadow: 0 10px 25px rgba(0,0,0,0.1); }
        .header { border-bottom: 5px solid var(--hadoop-yellow); padding-bottom: 15px; margin-bottom: 25px; display: flex; justify-content: space-between; align-items: center; }
        
        .set-nav { margin-bottom: 25px; display: flex; gap: 10px; flex-wrap: wrap; }
        .set-btn { padding: 12px 20px; background: #e9ecef; border: none; border-radius: 6px; cursor: pointer; text-decoration: none; color: #495057; font-weight: bold; transition: 0.3s; }
        .set-btn.active { background: var(--hadoop-blue); color: white; }
        .set-btn:hover:not(.active) { background: #dee2e6; }

        .question { margin-bottom: 25px; padding: 20px; border: 1px solid #dee2e6; border-radius: 10px; background: #fff; }
        .q-text { font-weight: bold; font-size: 1.15rem; display: block; margin-bottom: 15px; color: var(--hadoop-dark); }
        
        .option { display: block; padding: 14px; background: #f8f9fa; border: 2px solid #f1f2f6; margin: 8px 0; border-radius: 8px; cursor: pointer; transition: 0.2s; }
        .option:hover { background: #e9ecef; border-color: #dee2e6; }
        
        input[type="radio"] { margin-right: 10px; transform: scale(1.2); }

        .opt-correct { background: #d4edda !important; border-color: #28a745 !important; color: #155724; font-weight: bold; }
        .opt-wrong { background: #f8d7da !important; border-color: #dc3545 !important; color: #721c24; }
        
        .submit-btn { background: var(--hadoop-dark); color: white; border: none; padding: 20px; width: 100%; border-radius: 10px; font-size: 1.25rem; cursor: pointer; font-weight: bold; text-transform: uppercase; }
        
        #result-modal { display: none; position: fixed; top:0; left:0; width:100%; height:100%; background:rgba(0,0,0,0.8); z-index:999; justify-content:center; align-items:center; }
        .modal-box { background:white; padding:40px; border-radius:20px; text-align:center; max-width:500px; width: 90%; }
        .score-val { font-size: 4.5rem; color: var(--hadoop-blue); font-weight: 900; margin: 10px 0; }
    </style>
</head>
<body>

<div class="quiz-container">
    <div class="header">
        <div>
            <h1 style="margin:0;">M5: Hadoop Administration</h1>
            <p style="margin:5px 0 0; color: #6c757d;">Big Data Technologies • 40 Questions Per Set</p>
        </div>
        <a href="index.html" style="text-decoration:none; color:var(--hadoop-dark); font-weight:bold; border:2px solid var(--hadoop-dark); padding:8px 15px; border-radius:5px;">← Home</a>
    </div>

    <div class="set-nav">
        <a href="?set=1" class="set-btn" id="btn-1">Set 1: Architecture & HDFS</a>
        <a href="?set=2" class="set-btn" id="btn-2">Set 2: MapReduce & YARN</a>
        <a href="?set=3" class="set-btn" id="btn-3">Set 3: Installation & Config</a>
        <a href="?set=4" class="set-btn" id="btn-4">Set 4: Security & LDAP</a>
        <a href="?set=5" class="set-btn" id="btn-5">Set 5: Maintenance & Planning</a>
    </div>

    <form id="quiz-form">
        <div id="questions-area"></div>
        <button type="button" class="submit-btn" id="main-submit" onclick="validateAndScore()">Submit Set Results</button>
    </form>
</div>

<div id="result-modal">
    <div class="modal-box">
        <h2 id="result-title">Set Complete!</h2>
        <div class="score-val" id="score-display">0%</div>
        <p id="stats-summary" style="font-size: 1.3rem; margin-bottom:25px;"></p>
        <button onclick="location.reload()" style="padding:12px 25px; background:var(--hadoop-blue); color:white; border:none; border-radius:8px; cursor:pointer; font-weight:bold;">Retake Test</button>
        <button onclick="closeModal()" style="padding:12px 25px; margin-left:10px; cursor:pointer; background: #eee; border: none; border-radius: 8px;">Review Answers</button>
    </div>
</div>

<script>
const quizData = {
    "1": [
        { q: "What is the primary role of the NameNode in HDFS?", o: ["Stores actual data", "Manages file system metadata", "Executes Map tasks", "Acts as a backup for DataNodes"], a: 1 },
        { q: "What is the default block size in Hadoop 2.x/3.x?", o: ["64 MB", "128 MB", "256 MB", "512 MB"], a: 1 },
        { q: "Which daemon is responsible for managing block replication?", o: ["DataNode", "ResourceManager", "NameNode", "TaskTracker"], a: 2 },
        { q: "The Secondary NameNode is used to:", o: ["Provide High Availability", "Perform periodic checkpoints of the EditLog and FsImage", "Replace the NameNode if it fails", "Store data blocks"], a: 1 },
        { q: "How many copies of a block are stored by default in HDFS?", o: ["1", "2", "3", "4"], a: 2 },
        { q: "What does Rack Awareness ensure?", o: ["Data is only stored on one rack", "Data is distributed across different racks for fault tolerance", "Data is stored near the NameNode", "Reduces power consumption"], a: 1 },
        { q: "The 'FsImage' file contains:", o: ["Log of changes since last checkpoint", "The complete snapshot of the file system namespace", "Actual data blocks", "Configuration settings"], a: 1 },
        { q: "Which command is used to check the health of the HDFS file system?", o: ["hdfs dfs -ls", "hdfs fsck /", "hdfs datanode -check", "hadoop health"], a: 1 },
        { q: "DataNodes send ________ to the NameNode to signal they are active.", o: ["Handshakes", "Heartbeats", "Pings", "Tokens"], a: 1 },
        { q: "Which file system type is HDFS modeled after?", o: ["NTFS", "FAT32", "Unix/POSIX", "HFS+"], a: 2 },
        { q: "A 'Safe Mode' in NameNode means:", o: ["NameNode is in read-only mode and not replicating", "The cluster is shut down", "All data is being deleted", "DataNodes are writing data"], a: 0 },
        { q: "What happens if the NameNode's metadata is lost?", o: ["HDFS functions normally", "Data is safe but inaccessible", "The entire file system is lost", "Secondary NameNode automatically takes over"], a: 2 },
        { q: "The command to create a directory in HDFS is:", o: ["hdfs dfs -newdir", "hdfs dfs -mkdir", "hadoop fs -createdir", "hdfs mkdir"], a: 1 },
        { q: "In HDFS, files are:", o: ["Write-once, Read-many", "Write-many, Read-many", "Read-only", "Deleted after one read"], a: 0 },
        { q: "What is 'Balancing' in HDFS?", o: ["Evenly distributing data across DataNodes", "Splitting NameNode tasks", "Reducing replication", "Managing RAM"], a: 0 },
        { q: "Which daemon actually stores the data blocks?", o: ["NameNode", "DataNode", "JournalNode", "NodeManager"], a: 1 },
        { q: "The 'EditLog' stores:", o: ["Metadata snapshots", "Every change to the filesystem (deletes, adds, etc.)", "User passwords", "Hardware logs"], a: 1 },
        { q: "High Availability (HA) in Hadoop 2.0 uses which daemon for quorum?", o: ["Secondary NameNode", "JournalNodes", "ResourceManager", "DataNodes"], a: 1 },
        { q: "What is 'Federation' in HDFS?", o: ["Multiple NameNodes managing independent namespaces", "Using multiple DataNodes", "Connecting to AWS", "None"], a: 0 },
        { q: "Block reports are sent by:", o: ["NameNode to DataNode", "DataNode to NameNode", "Secondary NameNode to NameNode", "User to NameNode"], a: 1 },
        { q: "What is 'Data Locality'?", o: ["Moving data to the CPU", "Moving computation to where the data is stored", "Storing data in the cloud", "None"], a: 1 },
        { q: "Which component is responsible for client access to HDFS?", o: ["HDFS Client", "DataNode", "YARN", "MapReduce"], a: 0 },
        { q: "Short-circuit reads allow a client to:", o: ["Read from NameNode", "Read directly from local DataNode disk bypassing the DataNode daemon", "Skip security", "None"], a: 1 },
        { q: "HDFS is best suited for:", o: ["Low latency small file access", "High throughput large file processing", "Real-time database transactions", "OS installation"], a: 1 },
        { q: "The 'Secondary NameNode' is a hot standby.", o: ["True", "False"], a: 1 },
        { q: "Command to copy a file from local to HDFS:", o: ["hdfs dfs -put", "hdfs dfs -get", "hdfs dfs -cp", "hdfs dfs -move"], a: 0 },
        { q: "What is a 'Zombie DataNode'?", o: ["A dead node", "A node NameNode hasn't heard from in a while", "A node with no data", "None"], a: 1 },
        { q: "The HDFS block size is configurable.", o: ["True", "False"], a: 0 },
        { q: "What is the 'Checksum' used for in HDFS?", o: ["Data encryption", "Detecting data corruption", "Naming files", "None"], a: 1 },
        { q: "Which port is default for NameNode Web UI in Hadoop 3.x?", o: ["50070", "9870", "8088", "9000"], a: 1 },
        { q: "HDFS stands for:", o: ["Highly Distributed File System", "Hadoop Distributed File System", "Hard Disk File System", "None"], a: 1 },
        { q: "Can a user modify a block in place in HDFS?", o: ["Yes", "No", "Only if it is small", "Only with Root"], a: 1 },
        { q: "Replication Factor can be set at file level.", o: ["True", "False"], a: 0 },
        { q: "The 'distcp' command is for:", o: ["Local copy", "Distributed copy between clusters", "Deleting files", "None"], a: 1 },
        { q: "JournalNodes are part of which setup?", o: ["Single node", "High Availability NameNode", "Non-HA cluster", "YARN"], a: 1 },
        { q: "The command to see HDFS disk usage is:", o: ["hdfs dfs -du -h", "hdfs dfs -ls", "hadoop disk", "none"], a: 0 },
        { q: "What is the 'Trash' feature in HDFS?", o: ["Deletes files permanently", "Moves deleted files to a temporary folder for recovery", "Optimizes disk space", "None"], a: 1 },
        { q: "The NameNode is a single point of failure in Hadoop 1.x.", o: ["True", "False"], a: 0 },
        { q: "Maximum file size in HDFS is limited by:", o: ["Hard disk size", "RAM of NameNode", "DataNode count", "None"], a: 1 },
        { q: "What is 'Pipelining' in data writing?", o: ["Writing to all nodes at once", "Writing to 1st node, which sends to 2nd, which sends to 3rd", "Serial writing", "None"], a: 1 }
    ],
    "2": [
        { q: "What does YARN stand for?", o: ["Yet Another Resource Negotiator", "Your All Resource Network", "Yellow Apple Resource Node", "None"], a: 0 },
        { q: "Which YARN component is the ultimate authority in resource allocation?", o: ["NodeManager", "ResourceManager", "ApplicationMaster", "Container"], a: 1 },
        { q: "In MapReduce, the 'Shuffle and Sort' phase happens:", o: ["Before Map", "After Map, Before Reduce", "After Reduce", "During Map"], a: 1 },
        { q: "The 'ApplicationMaster' is responsible for:", o: ["Managing the entire cluster resources", "Managing the lifecycle of a specific application", "Storing HDFS data", "Managing local node resources"], a: 1 },
        { q: "What is a 'Container' in YARN?", o: ["A Docker container", "A resource abstraction (RAM, CPU) for a task", "An HDFS block", "A physical server"], a: 1 },
        { q: "Which daemon runs on every worker node in a YARN cluster?", o: ["ResourceManager", "NodeManager", "NameNode", "JobTracker"], a: 1 },
        { q: "The 'Mapper' task output is stored:", o: ["In HDFS", "On the local disk of the Mapper node", "In RAM only", "In the NameNode"], a: 1 },
        { q: "The 'Reducer' task output is stored:", o: ["In HDFS", "On local disk", "In NameNode", "On the client machine"], a: 0 },
        { q: "What is the purpose of the 'Combiner' in MapReduce?", o: ["To join two datasets", "To perform local aggregation to reduce network traffic", "To replace the Reducer", "None"], a: 1 },
        { q: "YARN separates which two functions of the old JobTracker?", o: ["Storage and Compute", "Resource Management and Job Scheduling/Monitoring", "Read and Write", "Metadata and Data"], a: 1 },
        { q: "Which scheduler is default in Hadoop (YARN)?", o: ["FIFO Scheduler", "Capacity Scheduler", "Fair Scheduler", "Priority Scheduler"], a: 1 },
        { q: "What is 'Speculative Execution'?", o: ["Executing code before writing", "Running a duplicate of a slow task on another node", "Predicting errors", "None"], a: 1 },
        { q: "The ApplicationMaster requests containers from:", o: ["NodeManager", "ResourceManager", "NameNode", "HDFS"], a: 1 },
        { q: "MapReduce is a ________ processing model.", o: ["Real-time", "Batch", "Interactive", "Stream"], a: 1 },
        { q: "What is the input to a Mapper?", o: ["A Key-Value pair", "A whole file", "A DataNode address", "A Reducer link"], a: 0 },
        { q: "Partitioning in MapReduce ensures:", o: ["Data is replicated", "All values for a specific key go to the same Reducer", "Maps run faster", "Disk is not full"], a: 1 },
        { q: "The 'Resource Calculator' in Capacity Scheduler usually looks at:", o: ["Disk Space", "Memory (RAM) and CPU Vcores", "Network Bandwidth", "User ID"], a: 1 },
        { q: "Which log would you check for a failed YARN application?", o: ["NameNode logs", "YARN Container/Application logs", "Syslog", "HDFS logs"], a: 1 },
        { q: "What is a 'Slot' in MapReduce v1?", o: ["A memory address", "A fixed unit of capacity for Map or Reduce tasks", "A network port", "None"], a: 1 },
        { q: "YARN allows multiple processing engines (Spark, Tez, etc.) to run on the same cluster.", o: ["True", "False"], a: 0 },
        { q: "The 'RecordReader' is responsible for:", o: ["Splitting the file", "Breaking a Split into individual records/lines for the Mapper", "Writing output", "None"], a: 1 },
        { q: "A 'Split' in MapReduce usually corresponds to:", o: ["One file", "One HDFS Block", "One whole rack", "None"], a: 1 },
        { q: "Can MapReduce run without YARN?", o: ["Yes (in v1)", "No", "Only on Windows", "Only on AWS"], a: 0 },
        { q: "The 'Timeline Service' in YARN is for:", o: ["Real-time clocks", "Storing application history and metrics", "Scheduling", "None"], a: 1 },
        { q: "What does 'Preemption' in YARN mean?", o: ["Starting a job", "Killing a lower-priority container to free resources for higher-priority jobs", "Stopping the cluster", "None"], a: 1 },
        { q: "Which command shows running YARN applications?", o: ["yarn application -list", "hadoop list", "mapred jobs", "yarn node -status"], a: 0 },
        { q: "The 'Fair Scheduler' aims to:", o: ["Give all jobs equal share of resources over time", "Run jobs in order of submission", "Prioritize admin jobs", "None"], a: 0 },
        { q: "What is 'Uberization' (Uber tasks) in YARN?", o: ["Using a taxi service", "Running small MapReduce tasks inside the ApplicationMaster to avoid overhead", "High speed maps", "None"], a: 1 },
        { q: "Which daemon starts the ApplicationMaster?", o: ["ResourceManager", "NodeManager", "DataNode", "User"], a: 0 },
        { q: "The 'Shuffle' phase involves:", o: ["Copying Mapper output to Reducers over the network", "Mixing data in HDFS", "Deleting temp files", "None"], a: 0 },
        { q: "MapReduce Job ID format usually starts with:", o: ["app_", "job_", "task_", "mr_"], a: 1 },
        { q: "What happens if a NodeManager fails?", o: ["Cluster shuts down", "ResourceManager marks it as lost and re-runs tasks on other nodes", "Data is lost", "None"], a: 1 },
        { q: "Can you run a Reduce-only job?", o: ["Yes", "No", "Only with 0 mappers", "None"], a: 1 },
        { q: "A 'Map-only' job has no:", o: ["Mappers", "Reducers", "Input", "Containers"], a: 1 },
        { q: "What is 'Distributed Cache'?", o: ["Web cache", "Facility to cache files needed by applications on worker nodes", "NameNode cache", "None"], a: 1 },
        { q: "ResourceManager has two main components: Scheduler and ________.", o: ["ApplicationManager", "NodeManager", "StorageManager", "SecurityManager"], a: 0 },
        { q: "The 'Node Labels' feature in YARN allows:", o: ["Naming nodes", "Partitioning cluster based on node characteristics (e.g., GPU vs CPU)", "Labeling users", "None"], a: 1 },
        { q: "The port for YARN Web UI is typically:", o: ["8088", "50070", "9870", "8042"], a: 0 },
        { q: "Which file configures the YARN daemons?", o: ["hdfs-site.xml", "yarn-site.xml", "mapred-site.xml", "core-site.xml"], a: 1 },
        { q: "In YARN, a job is called an ________.", o: ["Instance", "Application", "Process", "Thread"], a: 1 }
    ],
    "3": [
        { q: "Which file contains the global settings for the Hadoop framework?", o: ["core-site.xml", "hdfs-site.xml", "hadoop-env.sh", "masters"], a: 0 },
        { q: "Where do you define the NameNode address?", o: ["hdfs-site.xml", "core-site.xml (fs.defaultFS)", "yarn-site.xml", "slaves"], a: 1 },
        { q: "What is the purpose of 'hadoop-env.sh'?", o: ["Defining network ports", "Setting environment variables like JAVA_HOME", "Configuring block size", "None"], a: 1 },
        { q: "Which XML file is used to configure replication factor?", o: ["core-site.xml", "hdfs-site.xml", "yarn-site.xml", "mapred-site.xml"], a: 1 },
        { q: "To format the NameNode, you run:", o: ["hadoop format", "hdfs namenode -format", "hdfs -clear", "namenode init"], a: 1 },
        { q: "A 'Pseudo-Distributed' mode means:", o: ["Running on multiple clusters", "Running all Hadoop daemons on a single node", "Simulating Hadoop with Java", "None"], a: 1 },
        { q: "Which user is typically used to run Hadoop daemons?", o: ["root", "hadoop or hdfs", "administrator", "guest"], a: 1 },
        { q: "SSH password-less login is required for:", o: ["Security", "Automated startup and communication between nodes", "User login", "None"], a: 1 },
        { q: "The 'workers' (or 'slaves') file contains:", o: ["Admin names", "IPs/Hostnames of worker nodes", "File permissions", "None"], a: 1 },
        { q: "Which command starts all HDFS daemons?", o: ["start-all.sh", "start-dfs.sh", "hadoop start", "hdfs up"], a: 1 },
        { q: "What is the role of 'mapred-site.xml'?", o: ["Configure YARN", "Configure MapReduce parameters like framework name", "Configure HDFS", "None"], a: 1 },
        { q: "The property 'dfs.replication' is found in:", o: ["core-site.xml", "hdfs-site.xml", "slaves", "None"], a: 1 },
        { q: "Hadoop installation usually requires which Java version?", o: ["Java 1.0", "Java 8 or 11 (depends on version)", "JavaScript", "C++"], a: 1 },
        { q: "Which command starts YARN daemons?", o: ["start-yarn.sh", "start-all.sh", "yarn-up.sh", "resourcemanager start"], a: 0 },
        { q: "What is 'JPS' used for?", o: ["GPS for Hadoop", "Checking running Java processes/daemons", "Networking", "None"], a: 1 },
        { q: "Which directory usually stores HDFS data on DataNodes?", o: ["/bin", "/tmp", "Specified in hdfs-site.xml (dfs.datanode.data.dir)", "/etc/hadoop"], a: 2 },
        { q: "Default port for HDFS DataNode Web UI in 3.x?", o: ["50075", "9864", "8088", "None"], a: 1 },
        { q: "Before starting a new cluster, you must ________ the NameNode.", o: ["Delete", "Format", "Backup", "None"], a: 1 },
        { q: "Which file is used to set memory limits for ResourceManager?", o: ["yarn-site.xml", "core-site.xml", "hdfs-site.xml", "None"], a: 0 },
        { q: "What is 'Quorum Journal Manager' (QJM)?", o: ["A database", "A mechanism for HA NameNode shared storage", "A task scheduler", "None"], a: 1 },
        { q: "Property to set block size in hdfs-site.xml?", o: ["dfs.blocksize", "dfs.size", "hdfs.block", "none"], a: 0 },
        { q: "To stop HDFS, you use:", o: ["stop-dfs.sh", "shutdown -h now", "kill hdfs", "none"], a: 0 },
        { q: "Hadoop 3.x allows more than 2 NameNodes in HA.", o: ["True", "False"], a: 0 },
        { q: "The 'log4j.properties' file is for:", o: ["Network settings", "Logging configuration", "User accounts", "None"], a: 1 },
        { q: "Property 'fs.defaultFS' typically looks like:", o: ["hdfs://hostname:port", "/home/hadoop", "http://namenode", "none"], a: 0 },
        { q: "Which environment variable must be set in bashrc/profile?", o: ["HADOOP_HOME", "JAVA_HOME", "PATH", "All of the above"], a: 3 },
        { q: "A Multi-node cluster installation requires:", o: ["One PC only", "Multiple connected PCs via network", "No OS", "None"], a: 1 },
        { q: "Does Hadoop support Windows natively for production?", o: ["Yes", "No (mostly Linux/Unix)", "Only with Mac", "None"], a: 1 },
        { q: "The 'exclude' file in NameNode configuration is for:", o: ["Admin users", "Nodes to be decommissioned", "Corrupt files", "None"], a: 1 },
        { q: "The 'include' file is for:", o: ["Users", "Authorized worker nodes", "New files", "None"], a: 1 },
        { q: "Which command reloads the node list without restarting the NameNode?", o: ["hdfs dfsadmin -refreshNodes", "hadoop restart", "hdfs clear", "none"], a: 0 },
        { q: "Can you change 'dfs.replication' while Hadoop is running?", o: ["Yes", "No", "Only if you stop YARN", "None"], a: 0 },
        { q: "What is the purpose of 'capacity-scheduler.xml'?", o: ["Disk size", "Queue and resource allocation settings for YARN", "NameNode memory", "None"], a: 1 },
        { q: "The property 'yarn.nodemanager.resource.memory-mb' defines:", o: ["NameNode RAM", "Total RAM a NodeManager can give to containers", "Disk size", "None"], a: 1 },
        { q: "Hadoop 'Distributions' include:", o: ["Cloudera, Hortonworks (merged), MapR", "Windows, Linux, iOS", "Excel, Word", "None"], a: 0 },
        { q: "What is the default port for Secondary NameNode UI?", o: ["50090", "9868", "8088", "None"], a: 1 },
        { q: "Hadoop logs are usually found in:", o: ["/var/log/hadoop or $HADOOP_HOME/logs", "/tmp", "/etc", "none"], a: 0 },
        { q: "The 'slaves' file was renamed to ________ in Hadoop 3.x.", o: ["workers", "nodes", "members", "None"], a: 0 },
        { q: "Which tool is used for automated Hadoop installation/management?", o: ["Ambari", "Word", "Notepad", "Chrome"], a: 0 },
        { q: "What is a 'Cold Standby'?", o: ["Active node", "A node that needs manual intervention to take over (e.g. Hadoop 1.x Secondary)", "A frozen node", "None"], a: 1 }
    ],
    "4": [
        { q: "What is the primary security protocol used by Hadoop for authentication?", o: ["SSL", "Kerberos", "LDAP", "OAuth"], a: 1 },
        { q: "In Kerberos, what is a 'Ticket'?", o: ["A movie pass", "A temporary encrypted credential", "A file name", "None"], a: 1 },
        { q: "What does 'LDAP' stand for?", o: ["Lightweight Directory Access Protocol", "Large Data Access Path", "Linux Data Admin Process", "None"], a: 0 },
        { q: "What is 'HDFS Transparent Encryption'?", o: ["Visible passwords", "Encrypting data at rest without user changes", "Hiding files", "None"], a: 1 },
        { q: "A Kerberos 'Principal' represents:", o: ["A school head", "A unique identity (user or service) in Kerberos", "A data block", "None"], a: 1 },
        { q: "The 'KDC' in Kerberos stands for:", o: ["Key Distribution Center", "Kernel Data Center", "Key Delete Command", "None"], a: 0 },
        { q: "LDAP is often used in Hadoop for:", o: ["Storing data blocks", "User and group mapping/authentication", "Scheduling jobs", "None"], a: 1 },
        { q: "What is a 'Keytab' file?", o: ["A keyboard setting", "A file containing Kerberos principals and keys for automated login", "An HDFS log", "None"], a: 1 },
        { q: "HDFS 'ACLs' allow:", o: ["Faster reads", "Fine-grained permissions beyond standard POSIX (rwx)", "Deleting files", "None"], a: 1 },
        { q: "By default, Hadoop security is:", o: ["Strong", "Simple (OS user-based)", "Disabled", "Encrypted"], a: 1 },
        { q: "What is 'Delegation Token' in Hadoop?", o: ["A gift card", "A token that allows a service to act on behalf of a user", "A Map task", "None"], a: 1 },
        { q: "Apache Ranger is used for:", o: ["File storage", "Centralized security administration and auditing", "Monitoring RAM", "None"], a: 1 },
        { q: "Apache Knox provides:", o: ["Data compression", "Gateway security for Hadoop REST APIs", "SQL interface", "None"], a: 1 },
        { q: "Which command manages HDFS ACLs?", o: ["hdfs dfs -getfacl / setfacl", "chmod", "chown", "none"], a: 0 },
        { q: "What is a 'Realm' in Kerberos?", o: ["A kingdom", "An administrative boundary for authentication", "A Hadoop cluster", "None"], a: 1 },
        { q: "To enable Kerberos in Hadoop, you must set 'hadoop.security.authentication' to:", o: ["simple", "kerberos", "ldap", "none"], a: 1 },
        { q: "What is the role of 'kinit'?", o: ["Start Hadoop", "Authenticate a principal and get a ticket-granting ticket (TGT)", "Delete files", "None"], a: 1 },
        { q: "Encryption zones are created at which level?", o: ["File", "Directory", "Block", "Cluster"], a: 1 },
        { q: "The 'KMS' (Key Management Server) is used for:", o: ["Managing NameNode", "Managing encryption keys", "User passwords", "None"], a: 1 },
        { q: "Does LDAP store HDFS data?", o: ["Yes", "No", "Only if configured", "None"], a: 1 },
        { q: "What is 'Service Level Authorization'?", o: ["Authorizing users to use specific Hadoop services (e.g. MapReduce)", "Internet speed", "Hardware warranty", "None"], a: 0 },
        { q: "A 'TGT' (Ticket Granting Ticket) is used to:", o: ["Travel", "Request service tickets without re-entering password", "Format NameNode", "None"], a: 1 },
        { q: "Which property enables ACLs in HDFS?", o: ["dfs.namenode.acls.enabled", "hdfs.acl", "security.acl", "none"], a: 0 },
        { q: "Can Kerberos prevent a DataNode from joining the cluster?", o: ["No", "Yes (via Mutual Authentication)", "Only on Tuesdays", "None"], a: 1 },
        { q: "What is 'Audit Logging'?", o: ["Logging hardware errors", "Recording who accessed what data and when", "Storing Map output", "None"], a: 1 },
        { q: "Which file contains group mapping settings?", o: ["core-site.xml", "hdfs-site.xml", "security.xml", "none"], a: 0 },
        { q: "In a secure cluster, web UIs are protected by:", o: ["SPNEGO (Kerberos for HTTP)", "Simple passwords", "Nothing", "None"], a: 0 },
        { q: "Is 'root' a good user for Kerberos principals in Hadoop?", o: ["Yes", "No (Services should have their own principals)", "Maybe", "None"], a: 1 },
        { q: "LDAP 'Groups' can be used to control HDFS permissions.", o: ["True", "False"], a: 0 },
        { q: "What happens when a Kerberos ticket expires?", o: ["User is logged out/tasks fail unless renewed", "Nothing", "Password changes", "None"], a: 0 },
        { q: "Can a client access HDFS directly if Kerberos is enabled?", o: ["Yes", "No (Must authenticate first)", "Only if they know the IP", "None"], a: 1 },
        { q: "Hadoop 'Impersonation' (Proxy user) allows:", o: ["User A to act as User B with permissions", "Hacker access", "Changing NameNode", "None"], a: 0 },
        { q: "Which tool helps synchronize LDAP users to Hadoop?", o: ["SSSD or Hadoop's LdapGroupsMapping", "Excel", "Ftp", "None"], a: 0 },
        { q: "SSL/TLS in Hadoop is primarily for:", o: ["Data at rest", "Encryption of data in transit (over network)", "Naming files", "None"], a: 1 },
        { q: "Which property defines the Kerberos realm in core-site.xml?", o: ["hadoop.security.auth_to_local", "dfs.realm", "none", "none"], a: 0 },
        { q: "HDFS data blocks are encrypted by default.", o: ["True", "False"], a: 1 },
        { q: "What is 'Wire Encryption'?", o: ["Encrypting cables", "Encryption of RPC and HTTP traffic", "None", "None"], a: 1 },
        { q: "The command 'klist' shows:", o: ["All Hadoop files", "Currently held Kerberos tickets", "User list", "None"], a: 1 },
        { q: "Is LDAP a replacement for Kerberos?", o: ["Yes", "No (LDAP is often for identity/groups, Kerberos for auth)", "Maybe", "None"], a: 1 },
        { q: "Hadoop security is often called 'Project ________'.", o: ["Rhino", "Elephant", "Zebra", "Lion"], a: 0 }
    ],
    "5": [
        { q: "What is 'Decommissioning' a node?", o: ["Deleting data", "Gracefully removing a node from the cluster after replicating its data elsewhere", "Upgrading hardware", "None"], a: 1 },
        { q: "To decommission a DataNode, you add its name to:", o: ["include file", "exclude file", "hdfs-site.xml", "slaves"], a: 1 },
        { q: "Which command initiates the balancer?", o: ["hdfs balancer", "hadoop balance", "hdfs dfs -balance", "none"], a: 0 },
        { q: "What is a 'Snapshot' in HDFS?", o: ["A photo of the server", "A point-in-time read-only copy of the file system", "A backup on tape", "None"], a: 1 },
        { q: "How do you recover a deleted file if Trash is enabled?", o: ["You can't", "Move it from the .Trash folder back to original location", "Call tech support", "Format NameNode"], a: 1 },
        { q: "Which hardware is critical for the NameNode?", o: ["Many slow disks", "High RAM and redundant power/storage (SSD/RAID)", "GPU", "None"], a: 1 },
        { q: "DataNodes should ideally have:", o: ["Expensive RAID controllers", "JBOD (Just a Bunch Of Disks)", "Only 1GB RAM", "None"], a: 1 },
        { q: "What is 'Cluster Stress Testing' tool in Hadoop?", o: ["Word", "TestDFSIO", "Ping", "None"], a: 1 },
        { q: "Adding a new DataNode requires restarting the whole cluster.", o: ["True", "False"], a: 1 },
        { q: "How can you kill a YARN application?", o: ["yarn application -kill <appId>", "stop-yarn.sh", "kill -9 NameNode", "None"], a: 0 },
        { q: "What is 'Recommissioning'?", o: ["Buying new nodes", "Bringing a decommissioned node back into the cluster", "Deleting logs", "None"], a: 1 },
        { q: "Which metric is most important for HDFS health?", o: ["Number of Under-replicated blocks", "Number of users", "Color of the rack", "None"], a: 0 },
        { q: "The 'Metadata Backup' should be done for:", o: ["DataNode", "NameNode (FsImage/EditLogs)", "YARN", "None"], a: 1 },
        { q: "Hadoop 'Rolling Upgrades' allow:", o: ["Upgrading nodes one by one without total downtime", "Faster disks", "Changing colors", "None"], a: 0 },
        { q: "What is 'Rack Topology' configuration used for?", o: ["Improving cooling", "Fault tolerance and network performance optimization", "Naming racks", "None"], a: 1 },
        { q: "A 'Corrupt Block' is one that:", o: ["Has no data", "Fails the checksum test", "Is too large", "None"], a: 1 },
        { q: "Which tool is used for monitoring Hadoop metrics (open source)?", o: ["Ganglia / Nagios / Ambari Metrics", "Excel", "Photoshop", "None"], a: 0 },
        { q: "OS tuning for Hadoop includes:", o: ["Increasing file descriptors (ulimit)", "Disabling swapping (swappiness)", "Configuring Transparent Huge Pages", "All of the above"], a: 3 },
        { q: "What is the 'fsimage' and 'edits' merge process called?", o: ["Replication", "Checkpointing", "Balancing", "Formatting"], a: 1 },
        { q: "Can HDFS store billions of very small files efficiently?", o: ["Yes", "No (NameNode RAM will be exhausted)", "Only on SSD", "None"], a: 1 },
        { q: "What is 'Storage Policies' in HDFS?", o: ["User rules", "Moving data between different types of storage (SSD, Disk, Archive) based on age/usage", "Deleting data", "None"], a: 1 },
        { q: "A 'Heterogeneous Storage' cluster has:", o: ["Only one disk type", "Different types of storage like RAM, SSD, HDD, and Archive", "Multiple OS", "None"], a: 1 },
        { q: "What is 'Erasure Coding' in Hadoop 3.x?", o: ["Deleting code", "A more space-efficient way to provide fault tolerance than 3x replication", "Encrypting data", "None"], a: 1 },
        { q: "The command 'hdfs dfsadmin -report' gives summary of:", o: ["YARN jobs", "HDFS disk capacity and node status", "User passwords", "None"], a: 1 },
        { q: "Maximum number of DataNodes in a cluster is:", o: ["100", "1000", "Thousands (limited by NameNode RAM)", "Unlimited"], a: 2 },
        { q: "What is 'Speculative Execution' setting in mapred-site.xml?", o: ["Predicting results", "Toggling the running of duplicate tasks", "None", "None"], a: 1 },
        { q: "A 'Full' DataNode will:", o: ["Stop working", "Stop accepting new blocks (NameNode will route elsewhere)", "Delete old data", "None"], a: 1 },
        { q: "What is the impact of a high 'Swappiness' value on Hadoop?", o: ["Speeds up CPU", "Slows down daemons due to disk swapping", "Increases HDFS space", "None"], a: 1 },
        { q: "Benchmarks like 'TeraSort' are used for:", o: ["Sorting files", "Measuring end-to-end cluster performance", "Finding errors", "None"], a: 1 },
        { q: "What happens if the 'Secondary NameNode' fails?", o: ["Cluster fails", "Checkpointing stops, metadata grows too large over time", "Data is lost", "None"], a: 1 },
        { q: "The 'HDFS Balancer' threshold property defines:", o: ["Block size", "Percentage difference in disk usage allowed between nodes", "User count", "None"], a: 1 },
        { q: "Can you run Hadoop on a Cloud environment (AWS/Azure)?", o: ["Yes", "No", "Only if it is cold", "None"], a: 0 },
        { q: "What is 'Cold Data'?", o: ["Frozen data", "Data that is rarely accessed and can be moved to cheaper storage (Archive)", "New data", "None"], a: 1 },
        { q: "Does HDFS support hardware snapshots?", o: ["Yes", "No (Software-level snapshots)", "Only on Dell", "None"], a: 1 },
        { q: "What is the main challenge of 'Small Files' in Hadoop?", o: ["Large disk usage", "NameNode memory overhead (one object per file/block)", "Slow network", "None"], a: 1 },
        { q: "A 'Rack' usually holds how many servers in a typical data center?", o: ["1-2", "10-40", "100-200", "None"], a: 1 },
        { q: "The 'DataNode Volume Scanner' does:", o: ["Deletes data", "Periodically verifies all stored blocks for corruption", "Speeds up reads", "None"], a: 1 },
        { q: "What is 'Over-replication'?", o: ["Too many users", "More copies of a block than the set replication factor", "Fast writing", "None"], a: 1 },
        { q: "Which tool is used for migrating data from RDBMS to Hadoop?", o: ["Sqoop", "Flume", "Hive", "Pig"], a: 0 },
        { q: "What is the 'Apache' foundation?", o: ["A hardware company", "The open-source community managing Hadoop and its ecosystem", "A government body", "None"], a: 1 }
    ]
};

const urlParams = new URLSearchParams(window.location.search);
const currentSet = urlParams.get('set') || "1";
const questions = quizData[currentSet] || quizData["1"];

// Set active button
document.getElementById(`btn-${currentSet}`).classList.add('active');

// Render Questions
const area = document.getElementById('questions-area');
questions.forEach((q, i) => {
    let qDiv = document.createElement('div');
    qDiv.className = 'question';
    qDiv.id = `q-block-${i}`;
    
    let optionsHtml = q.o.map((opt, optIdx) => `
        <label class="option" id="opt-${i}-${optIdx}">
            <input type="radio" name="q${i}" value="${optIdx}">
            ${opt}
        </label>
    `).join('');

    qDiv.innerHTML = `
        <span class="q-text">${i + 1}. ${q.q}</span>
        ${optionsHtml}
    `;
    area.appendChild(qDiv);
});

function validateAndScore() {
    let score = 0;
    let unanswered = [];

    questions.forEach((q, i) => {
        const selected = document.querySelector(`input[name="q${i}"]:checked`);
        if (!selected) {
            unanswered.push(i + 1);
        }
    });

    if (unanswered.length > 0) {
        alert("Please answer all questions before submitting. Unanswered: " + unanswered.join(", "));
        return;
    }

    questions.forEach((q, i) => {
        const selected = parseInt(document.querySelector(`input[name="q${i}"]:checked`).value);
        const correct = q.a;

        // Reset styles
        const labels = document.querySelectorAll(`[id^="opt-${i}-"]`);
        labels.forEach(l => l.classList.remove('opt-correct', 'opt-wrong'));

        if (selected === correct) {
            score++;
            document.getElementById(`opt-${i}-${selected}`).classList.add('opt-correct');
        } else {
            document.getElementById(`opt-${i}-${selected}`).classList.add('opt-wrong');
            document.getElementById(`opt-${i}-${correct}`).classList.add('opt-correct');
        }
    });

    const finalScore = Math.round((score / questions.length) * 100);
    document.getElementById('score-display').innerText = finalScore + "%";
    document.getElementById('stats-summary').innerText = `You scored ${score} out of ${questions.length}`;
    document.getElementById('result-modal').style.display = 'flex';
}

function closeModal() {
    document.getElementById('result-modal').style.display = 'none';
    window.scrollTo({ top: 0, behavior: 'smooth' });
}
</script>

</body>
</html>