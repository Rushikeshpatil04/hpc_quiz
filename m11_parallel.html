<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 11: Parallel Computing - Master Quiz</title>
    <style>
        :root { --hpc-blue: #0984e3; --hpc-dark: #2d3436; --bg: #f5f6fa; --success: #00b894; --wrong: #d63031; }
        body { font-family: 'Segoe UI', Arial, sans-serif; background: var(--bg); color: var(--hpc-dark); padding: 20px; }
        .quiz-container { max-width: 1000px; margin: auto; background: white; padding: 30px; border-radius: 12px; box-shadow: 0 10px 30px rgba(0,0,0,0.1); }
        .header { border-bottom: 4px solid var(--hpc-blue); padding-bottom: 15px; margin-bottom: 25px; display: flex; justify-content: space-between; align-items: center; }
        .back-link { text-decoration: none; color: var(--hpc-blue); font-weight: bold; border: 2px solid var(--hpc-blue); padding: 8px 18px; border-radius: 6px; transition: 0.3s; }
        .back-link:hover { background: var(--hpc-blue); color: white; }
        
        .set-nav { margin-bottom: 25px; display: flex; gap: 10px; flex-wrap: wrap; }
        .set-btn { padding: 10px 20px; background: #dfe6e9; border: none; border-radius: 5px; cursor: pointer; text-decoration: none; color: black; font-weight: bold; transition: 0.2s; }
        .set-btn.active { background: var(--hpc-blue); color: white; }

        .question { margin-bottom: 25px; padding: 20px; border: 1px solid #ddd; border-radius: 8px; background: #fff; }
        .question.unanswered { border-left: 8px solid var(--wrong); background: #fff5f5; }
        .q-text { font-weight: bold; font-size: 1.1rem; display: block; margin-bottom: 15px; color: #2d3436; }
        .option { display: block; padding: 12px; background: #f9f9f9; border: 1px solid #eee; margin: 8px 0; border-radius: 6px; cursor: pointer; transition: 0.2s; }
        .option:hover { background: #f1f2f6; border-color: var(--hpc-blue); }
        
        .submit-btn { background: var(--success); color: white; border: none; padding: 20px; width: 100%; border-radius: 8px; font-size: 1.3rem; cursor: pointer; font-weight: bold; margin-top: 10px; }
        
        #result-modal { display: none; position: fixed; top:0; left:0; width:100%; height:100%; background:rgba(0,0,0,0.85); z-index:999; justify-content:center; align-items:center; }
        .modal-box { background:white; padding:40px; border-radius:15px; text-align:center; max-width:450px; width: 90%; }
        .score-val { font-size: 4rem; color: var(--hpc-blue); font-weight: bold; margin: 15px 0; }
    </style>
</head>
<body>

<div class="quiz-container">
    <div class="header">
        <div>
            <h1 style="margin:0;">Parallel Computing Mastery</h1>
            <p style="margin:5px 0 0; color: #636e72;">MPI, OpenMP, CUDA, and Parallel Performance</p>
        </div>
        <a href="index.html" class="back-link">‚Üê Dashboard</a>
    </div>

    <div class="set-nav">
        <a href="?set=1" class="set-btn" id="btn-1">Set 1</a>
        <a href="?set=2" class="set-btn" id="btn-2">Set 2</a>
        <a href="?set=3" class="set-btn" id="btn-3">Set 3</a>
        <a href="?set=4" class="set-btn" id="btn-4">Set 4</a>
        <a href="?set=5" class="set-btn" id="btn-5">Set 5</a>
    </div>

    

    <form id="quiz-form">
        <div id="questions-area"></div>
        <button type="button" class="submit-btn" onclick="validateQuiz()">Submit This Set</button>
    </form>
</div>

<div id="result-modal">
    <div class="modal-box">
        <h2>Set Complete!</h2>
        <div class="score-val" id="score-display">0%</div>
        <div id="stats-summary" style="margin-bottom:20px; font-size: 1.2rem;"></div>
        <button onclick="location.reload()" style="padding:10px 20px; background:var(--hpc-blue); color:white; border:none; border-radius:5px; cursor:pointer;">Retake Set</button>
        <button onclick="closeModal()" style="padding:10px 20px; margin-left:10px; cursor:pointer; background: #eee; border: none; border-radius: 5px;">Review</button>
    </div>
</div>

<script>
const quizData = {
    "1": [
        { q: "What does MPI stand for?", o: ["Message Processing Interface", "Message Passing Interface", "Massive Parallel Integration", "None"], a: 1 },
        { q: "OpenMP is primarily used for which type of architecture?", o: ["Distributed Memory", "Shared Memory", "Vector Processing", "Hybrid only"], a: 1 },
        { q: "Which law defines the theoretical speedup of a program using multiple processors?", o: ["Moore's Law", "Amdahl's Law", "Gustafson's Law", "Newton's Law"], a: 1 },
        { q: "In MPI, which rank is typically designated as the 'Master' process?", o: ["Rank 1", "Rank -1", "Rank 0", "Any random rank"], a: 2 },
        { q: "Which OpenMP directive is used to parallelize a 'for' loop?", o: ["#pragma omp loop", "#pragma omp parallel for", "#pragma omp parallel", "#pragma omp task"], a: 1 },
        { q: "What is 'Speedup' in parallel computing?", o: ["T_serial / T_parallel", "T_parallel / T_serial", "T_serial + T_parallel", "None"], a: 0 },
        { q: "Which of the following is a distributed memory system?", o: ["Multicore PC", "Cluster of Workstations", "Symmetric Multiprocessor (SMP)", "GPU"], a: 1 },
        { q: "In MPI, which function is used to initialize the MPI environment?", o: ["MPI_Start()", "MPI_Init()", "MPI_Begin()", "MPI_Setup()"], a: 1 },
        { q: "What is a 'Race Condition'?", o: ["A fast algorithm", "Multiple threads accessing shared data simultaneously where at least one is a write", "A network competition", "None"], a: 1 },
        { q: "Which MPI function sends a message to all other processes in a communicator?", o: ["MPI_Send", "MPI_Bcast", "MPI_Scatter", "MPI_Gather"], a: 1 },
        { q: "What does 'CUDA' stand for?", o: ["Computer Unified Digital Architecture", "Compute Unified Device Architecture", "Core Unit Data Access", "None"], a: 1 },
        { q: "In parallel computing, 'Granularity' refers to:", o: ["Size of the processor", "Ratio of computation to communication", "Hard drive speed", "Memory size"], a: 1 },
        { q: "What is 'Deadlock'?", o: ["A computer crash", "Two or more processes waiting indefinitely for each other to release resources", "A broken network cable", "None"], a: 1 },
        { q: "Which MPI function is used to stop the MPI environment?", o: ["MPI_Stop()", "MPI_End()", "MPI_Finalize()", "MPI_Close()"], a: 2 },
        { q: "A 'Critical Section' in OpenMP is:", o: ["A section that must be skipped", "A block of code that must be executed by only one thread at a time", "A section for errors", "None"], a: 1 },
        { q: "What is 'Scalability'?", o: ["The size of the cluster", "Ability of a system to maintain performance as processors are added", "The height of the server rack", "None"], a: 1 },
        { q: "Which law focuses on the speedup of a parallel system with a fixed time?", o: ["Amdahl's Law", "Gustafson's Law", "Moore's Law", "Metcalfe's Law"], a: 1 },
        { q: "In MPI, 'Comm_size' refers to:", o: ["Size of a message", "Total number of processes in a communicator", "Memory size", "Number of nodes"], a: 1 },
        { q: "What is a 'Barrier' in parallel programming?", o: ["A firewall", "A synchronization point where all processes must wait before proceeding", "A memory limit", "None"], a: 1 },
        { q: "What is 'SIMD'?", o: ["Single Instruction Multiple Data", "Simple Integration Multiple Devices", "Shared Internal Memory Data", "None"], a: 0 },
        { q: "Which of the following is an example of 'Embarrassingly Parallel'?", o: ["Weather forecasting", "Brute-force password cracking", "Matrix inversion", "Fluid dynamics"], a: 1 },
        { q: "In OpenMP, which clause makes a variable private to each thread?", o: ["shared", "private", "firstprivate", "lastprivate"], a: 1 },
        { q: "What is 'Latency' in interconnection networks?", o: ["Data transfer rate", "Time taken for a single bit to travel from source to destination", "Network cost", "None"], a: 1 },
        { q: "What is 'Bandwidth'?", o: ["Length of a cable", "Maximum rate of data transfer across a network", "Frequency of CPU", "None"], a: 1 },
        { q: "In MPI, which function collects data from all processes to a single master process?", o: ["MPI_Scatter", "MPI_Gather", "MPI_Bcast", "MPI_Reduce"], a: 1 },
        { q: "In CUDA, what is a 'Kernel'?", o: ["The OS core", "A function that runs on the GPU", "A memory type", "None"], a: 1 },
        { q: "Which term describes a system where all processors share a single address space?", o: ["NUMA", "UMA (Uniform Memory Access)", "Message Passing", "Grid"], a: 1 },
        { q: "What is 'NUMA'?", o: ["Non-Uniform Memory Access", "Network Unit Management Area", "New Unit Memory Architecture", "None"], a: 0 },
        { q: "In MPI, what is 'MPI_COMM_WORLD'?", o: ["A global variable", "The default communicator including all processes", "A network protocol", "None"], a: 1 },
        { q: "Which OpenMP directive ensures a block of code is executed by only the master thread?", o: ["#pragma omp master", "#pragma omp single", "#pragma omp only", "None"], a: 0 },
        { q: "What is 'False Sharing'?", o: ["Sharing data with neighbors", "Performance degradation where threads on different cores update variables in same cache line", "Copying files incorrectly", "None"], a: 1 },
        { q: "In CUDA, what is a 'Thread Block'?", o: ["A group of CPU threads", "A collection of GPU threads that can cooperate", "A memory error", "None"], a: 1 },
        { q: "What is 'MIMD'?", o: ["Multiple Instruction Multiple Data", "Main Internal Memory Device", "Massive Integration Managed Data", "None"], a: 0 },
        { q: "Which MPI function performs a global reduction (e.g., sum) and stores the result on all processes?", o: ["MPI_Reduce", "MPI_Allreduce", "MPI_Sum", "MPI_Gather"], a: 1 },
        { q: "What is the 'Efficiency' of a parallel system?", o: ["Speedup / Number of Processors", "Number of Processors / Speedup", "Total Time / Parallel Time", "None"], a: 0 },
        { q: "In OpenMP, 'OMP_NUM_THREADS' is used to:", o: ["Set the number of processes", "Environment variable to set the number of threads", "A function to count cores", "None"], a: 1 },
        { q: "What is 'Data Decomposition'?", o: ["Deleting data", "Dividing data among different processors to work on it in parallel", "Compressing data", "None"], a: 1 },
        { q: "In MPI, which call sends a message without blocking the sender?", o: ["MPI_Send", "MPI_Isend", "MPI_Bsend", "MPI_Ssend"], a: 1 },
        { q: "What is 'Dynamic Scheduling' in OpenMP?", o: ["Allocating tasks at compile time", "Assigning iterations to threads at runtime based on availability", "Changing CPU speed", "None"], a: 1 },
        { q: "The 'Top500' list ranks supercomputers based on which benchmark?", o: ["SPECint", "LINPACK", "CoreMark", "3DMark"], a: 1 }
    ],
    "2": [
        { q: "Which type of parallelism does a GPU primarily exploit?", o: ["Task Parallelism", "Data Parallelism", "Instruction Parallelism", "None"], a: 1 },
        { q: "What is 'Shared Memory'?", o: ["Memory shared on the cloud", "Memory that can be accessed by all processors in a system", "A file sharing protocol", "None"], a: 1 },
        { q: "What is 'Distributed Memory'?", o: ["Memory spread across a disk", "Each processor has its own private memory; communication via messages", "RAM shared via Wi-Fi", "None"], a: 1 },
        { q: "In MPI, 'MPI_Scatter' does what?", o: ["Collects data", "Splits a buffer from one process and sends pieces to all processes", "Sends same data to everyone", "None"], a: 1 },
        { q: "What is a 'Warp' in NVIDIA CUDA?", o: ["A space travel term", "A group of 32 threads executed simultaneously", "A type of GPU cable", "None"], a: 1 },
        { q: "Which OpenMP clause ensures all threads start with the initial value of the original variable?", o: ["private", "firstprivate", "lastprivate", "shared"], a: 1 },
        { q: "What is 'Strong Scaling'?", o: ["Fixed problem size, increasing processors", "Increasing problem size and processors", "Higher CPU frequency", "None"], a: 0 },
        { q: "What is 'Weak Scaling'?", o: ["Fixed problem size per processor", "Fixed total problem size", "Lower CPU frequency", "None"], a: 0 },
        { q: "Which MPI function is used to find the rank of the current process?", o: ["MPI_Comm_rank", "MPI_Get_id", "MPI_Rank", "MPI_Whoami"], a: 0 },
        { q: "What is 'Thread Affinity'?", o: ["Threads liking each other", "Binding a thread to a specific CPU core", "Thread synchronization", "None"], a: 1 },
        { q: "Which tool is used to monitor GPU utilization in NVIDIA systems?", o: ["top", "nvidia-smi", "htop", "glance"], a: 1 },
        { q: "What is 'Hyper-threading'?", o: ["Connecting nodes with threads", "Executing two threads on a single physical core to improve throughput", "A network protocol", "None"], a: 1 },
        { q: "What does 'GPGPU' stand for?", o: ["General Purpose Graphics Processing Unit", "Global Parallel GPU", "Graphic Processing Unit", "None"], a: 0 },
        { q: "In OpenMP, which schedule type is best for loops with highly irregular workloads?", o: ["static", "dynamic", "guided", "runtime"], a: 1 },
        { q: "Which MPI function combines data from all processes and leaves the result on one process?", o: ["MPI_Gather", "MPI_Reduce", "MPI_Bcast", "MPI_Scan"], a: 1 },
        { q: "What is 'Throughput'?", o: ["Delay in network", "The amount of work completed in a given time", "Speed of light", "None"], a: 1 },
        { q: "Which protocol is standard for MPI communication over InfiniBand?", o: ["TCP/IP", "RDMA (Remote Direct Memory Access)", "HTTP", "FTP"], a: 1 },
        { q: "In CUDA, what is 'Global Memory'?", o: ["Memory on the CPU", "DRAM on the GPU accessible by all blocks", "Cache on the disk", "None"], a: 1 },
        { q: "What is the 'Implicit Barrier' in OpenMP?", o: ["A barrier you must code", "A synchronization point automatically added at the end of parallel regions", "A firewall", "None"], a: 1 },
        { q: "Which benchmark is used to measure the power efficiency of supercomputers?", o: ["Top500", "Green500", "Graph500", "EnergyMark"], a: 1 },
        { q: "What is 'Vectorization'?", o: ["Converting code to Python", "Using a single instruction to process multiple data elements (SIMD)", "Drawing graphics", "None"], a: 1 },
        { q: "In MPI, 'MPI_Wait' is used for what?", o: ["Waiting for a node to boot", "Blocking until a specific non-blocking operation completes", "Pausing the entire cluster", "None"], a: 1 },
        { q: "What is 'Coalesced' memory access in GPUs?", o: ["Random access", "Combining multiple memory requests from threads into one transaction", "Deleting memory", "None"], a: 1 },
        { q: "Which library is the standard for high-performance matrix operations?", o: ["Libc", "BLAS (Basic Linear Algebra Subprograms)", "OpenSSL", "FFTW"], a: 1 },
        { q: "What is 'Heterogeneous Computing'?", o: ["Using only CPUs", "Using systems with more than one kind of processor (e.g., CPU + GPU)", "Using different brands of RAM", "None"], a: 1 },
        { q: "In OpenMP, 'OMP_STACKSIZE' is used to control what?", o: ["Number of CPUs", "The size of the stack for each thread", "Disk space", "None"], a: 1 },
        { q: "Which MPI function is used to broadcast data from one process to all others?", o: ["MPI_Send", "MPI_Bcast", "MPI_Scatter", "MPI_Alltoall"], a: 1 },
        { q: "What is 'Instruction Latency'?", o: ["The speed of the clock", "The number of cycles it takes to execute an instruction", "The size of a file", "None"], a: 1 },
        { q: "Which interconnection topology connects nodes in a grid or cube pattern?", o: ["Ring", "Mesh/Torus", "Star", "Bus"], a: 1 },
        { q: "What is 'Pthreads'?", o: ["Python threads", "POSIX threads (low-level threading API for C/C++)", "Parallel threads", "None"], a: 1 },
        { q: "Which MPI function allows a process to send data to itself?", o: ["Not possible", "MPI_Send (to its own rank)", "MPI_Bcast", "None"], a: 1 },
        { q: "What is 'Amdahl's Law' primarily concerned with?", o: ["Total system cost", "The serial fraction of a program limiting speedup", "Network latency", "None"], a: 1 },
        { q: "In CUDA, what is a 'Streaming Multiprocessor' (SM)?", o: ["A type of RAM", "A unit on the GPU that contains multiple CUDA cores", "A network switch", "None"], a: 1 },
        { q: "What is 'Data Locality'?", o: ["Keeping data on a local disk", "Keeping data as close as possible to the processing core that needs it", "Encryption", "None"], a: 1 },
        { q: "Which MPI function is 'Non-blocking Receive'?", o: ["MPI_Recv", "MPI_Irecv", "MPI_Get", "MPI_Pull"], a: 1 },
        { q: "What is 'Communication-to-Computation Ratio'?", o: ["Network speed vs CPU speed", "Time spent communicating vs time spent calculating", "The cost of the cluster", "None"], a: 1 },
        { q: "Which OpenMP clause allows variables to retain their value across parallel regions?", o: ["private", "threadprivate", "shared", "static"], a: 1 },
        { q: "What is 'SIMT' in NVIDIA architecture?", o: ["Single Instruction Multiple Tasks", "Single Instruction Multiple Threads", "Synchronized Internal Memory Transfer", "None"], a: 1 },
        { q: "Which MPI function performs an element-wise reduction and scatters result to all processes?", o: ["MPI_Reduce", "MPI_Allreduce", "MPI_Scan", "MPI_Gather"], a: 1 },
        { q: "What is 'Pipelining' in a CPU?", o: ["A cooling system", "Executing multiple instructions in different stages of completion simultaneously", "A type of storage", "None"], a: 1 }
    ],
    "3": [
        { q: "What is the 'Flynn's Taxonomy' classification for a standard modern supercomputer cluster?", o: ["SISD", "SIMD", "MISD", "MIMD"], a: 3 },
        { q: "In MPI, which function is used to wait for all processes in a communicator to reach the same point?", o: ["MPI_Wait", "MPI_Barrier", "MPI_Sync", "MPI_Hold"], a: 1 },
        { q: "Which CUDA memory type is visible to all threads in a block and has low latency?", o: ["Global Memory", "Shared Memory", "Constant Memory", "Texture Memory"], a: 1 },
        { q: "What is 'Speedup' if a program takes 100s on 1 core and 25s on 5 cores?", o: ["2", "4", "5", "25"], a: 1 },
        { q: "Which OpenMP clause allows threads to perform a reduction (like summation) into a single variable?", o: ["sum", "combine", "reduction", "gather"], a: 2 },
        { q: "What is 'Computational Intensity'?", o: ["CPU temperature", "Ratio of arithmetic operations to memory accesses", "Number of users on a node", "None"], a: 1 },
        { q: "In MPI, the parameter 'tag' in MPI_Send is used for:", o: ["Identifying the process", "Distinguishing between different messages", "Naming the cluster", "Setting priority"], a: 1 },
        { q: "Which interconnection topology connects every node to every other node?", o: ["Ring", "Mesh", "Fully Connected (All-to-All)", "Hypercube"], a: 2 },
        { q: "What is 'GPGPU' stand for?", o: ["General Purpose Graphics Processing Unit", "Global Parallel GPU", "Generic Processor Unit", "None"], a: 0 },
        { q: "In OpenMP, the 'nowait' clause is used to:", o: ["Stop the program", "Skip the implicit barrier at the end of a work-sharing region", "Force a reboot", "None"], a: 1 },
        { q: "What is a 'Compute Capability' in NVIDIA GPUs?", o: ["Total RAM", "Version of the GPU architecture features", "Clock speed", "Number of fans"], a: 1 },
        { q: "Which MPI function is used to send a message to a specific process and receive one back simultaneously?", o: ["MPI_Bcast", "MPI_Sendrecv", "MPI_Alltoall", "MPI_Scatter"], a: 1 },
        { q: "What is 'Loop Splitting'?", o: ["Breaking a loop into independent parts for parallel execution", "Deleting a loop", "Infinitely running a loop", "None"], a: 0 },
        { q: "Which term describes a parallel system where the number of processors can be increased indefinitely without performance loss?", o: ["Perfectly Scalable", "Linear", "Ideal", "All of these"], a: 3 },
        { q: "What is 'MPI_ANY_SOURCE' used for?", o: ["To send to everyone", "To receive a message from any process rank", "To find any node", "None"], a: 1 },
        { q: "In CUDA, what is the 'Host'?", o: ["The GPU", "The CPU and its memory", "The Network Switch", "The User"], a: 1 },
        { q: "In CUDA, what is the 'Device'?", o: ["The CPU", "The GPU and its memory", "The Monitor", "The Hard Drive"], a: 1 },
        { q: "What is 'Coalesced Memory Access' in GPU computing?", o: ["Random memory access", "Combining multiple memory requests into a single transaction", "Blocking memory access", "None"], a: 1 },
        { q: "Which OpenMP directive allows a specific block of code to be executed by only one thread at a time, but not necessarily the master?", o: ["#pragma omp master", "#pragma omp critical", "#pragma omp single", "#pragma omp barrier"], a: 1 },
        { q: "In MPI, what is 'Deadlock' usually caused by?", o: ["Too much memory", "Cyclic dependency of blocking Send/Recv calls", "Fast CPU", "None"], a: 1 },
        { q: "What is 'Communication Overhead'?", o: ["Cost of buying routers", "Time spent by processors communicating instead of computing", "Electric bill", "None"], a: 1 },
        { q: "Which MPI function provides the total number of processes in a communicator?", o: ["MPI_Comm_rank", "MPI_Comm_size", "MPI_Total", "MPI_Count"], a: 1 },
        { q: "What is 'Hybrid Parallelism'?", o: ["Using two clusters", "Combining two models (e.g., MPI + OpenMP) in one application", "Using Windows and Linux", "None"], a: 1 },
        { q: "In OpenMP, which schedule type divides iterations into fixed-sized chunks assigned to threads in round-robin?", o: ["static", "dynamic", "guided", "runtime"], a: 0 },
        { q: "What is 'Message Passing'?", o: ["Writing to a file", "Method of communication where processes exchange data via explicit Send/Recv", "Sharing a database", "None"], a: 1 },
        { q: "Which of these is a 'Blocking' communication?", o: ["MPI_Isend", "MPI_Send", "MPI_Irecv", "None"], a: 1 },
        { q: "What is 'Task Parallelism'?", o: ["Doing one task on many data points", "Distributing different tasks (functions) across different processors", "Scheduling a cron job", "None"], a: 1 },
        { q: "What is 'Data Parallelism'?", o: ["Distributing the same task across different subsets of data", "Copying data", "Deleting data", "None"], a: 0 },
        { q: "In MPI, 'MPI_REDUCE' with 'MPI_MAX' operation does what?", o: ["Sums all values", "Finds the maximum value among all processes", "Finds the average", "None"], a: 1 },
        { q: "What is 'Pthreads'?", o: ["Python threads", "POSIX Threads (a low-level C/C++ library for threading)", "Parallel threads", "None"], a: 1 },
        { q: "Which MPI function is used to create a new communicator from a subset of processes?", o: ["MPI_Comm_split", "MPI_Comm_new", "MPI_Comm_group", "MPI_Comm_make"], a: 0 },
        { q: "What is 'Cache Coherency'?", o: ["Cleaning the cache", "Ensuring all processors see the same data value in their local caches", "Increasing cache size", "None"], a: 1 },
        { q: "In CUDA, what is 'Global Memory'?", o: ["Cache on CPU", "Main memory on the GPU accessible by all threads", "RAM on the motherboard", "None"], a: 1 },
        { q: "What is 'SIMT' in NVIDIA architecture?", o: ["Single Instruction Multiple Threads", "Simple Internal Memory Transfer", "Synchronized Interface Management", "None"], a: 0 },
        { q: "Which OpenMP function returns the ID of the calling thread?", o: ["omp_get_thread_num()", "omp_get_num_threads()", "omp_id()", "omp_thread_id()"], a: 0 },
        { q: "Which OpenMP function returns the total number of threads in the current team?", o: ["omp_get_thread_num()", "omp_get_num_threads()", "omp_count()", "None"], a: 1 },
        { q: "What is 'Inter-process Communication' (IPC)?", o: ["Talking to a person", "Mechanisms provided by OS to allow processes to manage shared data", "Buying new CPUs", "None"], a: 1 },
        { q: "In MPI, what happens to the ranks if you have 4 processes?", o: ["1, 2, 3, 4", "0, 1, 2, 3", "A, B, C, D", "None"], a: 1 },
        { q: "What is 'Data Locality'?", o: ["Keeping data on a USB", "Arranging data so it is close to the processor that needs it", "Geographic location of servers", "None"], a: 1 },
        { q: "Which tool is commonly used to profile MPI applications?", o: ["Vampir / Tau", "Vim", "Ping", "Nmap"], a: 0 }
    ],
    "4": [
        { q: "Which MPI call sends data from every process to every process?", o: ["MPI_Alltoall", "MPI_Bcast", "MPI_Scatter", "MPI_Gather"], a: 0 },
        { q: "In OpenMP, which clause specifies that a variable should be shared among all threads?", o: ["private", "shared", "all", "common"], a: 1 },
        { q: "What is 'Thread Safety'?", o: ["Using a password", "Code that functions correctly during simultaneous execution by multiple threads", "A secure network", "None"], a: 1 },
        { q: "Which CUDA function qualifier specifies that a function runs on the device and is called from the host?", o: ["__device__", "__global__", "__host__", "__kernel__"], a: 1 },
        { q: "What is 'Instruction-Level Parallelism' (ILP)?", o: ["Many CPUs working", "Parallelism within a single processor core (pipelining, superscalar)", "A network of PCs", "None"], a: 1 },
        { q: "In MPI, what is a 'Communicator'?", o: ["A physical cable", "A logical group of processes that can talk to each other", "A user", "None"], a: 1 },
        { q: "What is 'Load Balancing'?", o: ["Equal weight on server racks", "Distributing work evenly across processors to avoid idle time", "Buying more RAM", "None"], a: 1 },
        { q: "Which OpenMP environment variable controls whether nested parallelism is enabled?", o: ["OMP_NESTED", "OMP_PARALLEL", "OMP_THREAD_LIMIT", "OMP_STACKSIZE"], a: 0 },
        { q: "What is 'Granularity' in the context of task size?", o: ["Small tasks = Fine-grained, Large tasks = Coarse-grained", "Small CPUs = Fine-grained", "Heavy data = Coarse-grained", "None"], a: 0 },
        { q: "Which type of memory access pattern is generally fastest?", o: ["Random", "Sequential / Contiguous", "Strided", "Reverse"], a: 1 },
        { q: "What is 'MPI_Barrier' used for?", o: ["To measure speed", "To synchronize all processes in a communicator", "To stop a program", "None"], a: 1 },
        { q: "In OpenMP, what is the 'TEAM' of threads?", o: ["A sports group", "The set of threads created by a parallel region", "A network subnet", "None"], a: 1 },
        { q: "Which library provides high-performance communication for GPUs?", o: ["MPI", "NCCL (NVIDIA Collective Communications Library)", "libc", "OpenSSL"], a: 1 },
        { q: "What is 'SpMV'?", o: ["A type of video", "Sparse Matrix-Vector Multiplication (a key HPC kernel)", "Speedy Multi-vector", "None"], a: 1 },
        { q: "Which MPI function provides the name of the processor the process is running on?", o: ["MPI_Get_processor_name", "MPI_Whoami", "MPI_Hostname", "None"], a: 0 },
        { q: "What is 'False Sharing' caused by?", o: ["Two CPUs sharing a monitor", "Multiple threads modifying variables on the same cache line", "Copying a file twice", "None"], a: 1 },
        { q: "Which OpenMP directive is used for mutual exclusion?", o: ["#pragma omp parallel", "#pragma omp critical", "#pragma omp for", "None"], a: 1 },
        { q: "What is 'Roofline Model'?", o: ["A blueprint for a building", "A visual performance model used to identify bottlenecks", "A cooling system", "None"], a: 1 },
        { q: "In MPI, what is 'Collective Communication'?", o: ["Talking one-to-one", "Operations involving all processes in a communicator", "Sending an email", "None"], a: 1 },
        { q: "What is 'FLOPS'?", o: ["Flops of a bird", "Floating Point Operations Per Second", "A failure in system", "None"], a: 1 },
        { q: "What is 'TFLOPS'?", o: ["1000 FLOPS", "Tera-FLOPS (1 Trillion FLOPS)", "Ten FLOPS", "None"], a: 1 },
        { q: "What is 'PFLOPS'?", o: ["1000 TFLOPS", "Peta-FLOPS (1 Quadrillion FLOPS)", "Power FLOPS", "None"], a: 1 },
        { q: "Which supercomputer was the first to reach the Exascale (EFLOPS)?", o: ["Summit", "Fugaku", "Frontier", "Tianhe-2"], a: 2 },
        { q: "What is 'Shared Memory' on a GPU?", o: ["VRAM", "On-chip memory accessible by threads within the same block", "System RAM", "None"], a: 1 },
        { q: "In MPI, 'MPI_Gather' collects data to which process by default?", o: ["All processes", "The root process", "Random process", "None"], a: 1 },
        { q: "What is 'Strong Scaling' efficiency?", o: ["Speedup / N (where N is processors)", "Constant", "Always 100%", "None"], a: 0 },
        { q: "In OpenMP, what is 'Static' schedule?", o: ["Assigned at runtime", "Assigned at compile-time/start-up in fixed chunks", "Random", "None"], a: 1 },
        { q: "What is 'Atomic' operation in OpenMP?", o: ["Using nuclear energy", "Ensuring a memory location is updated by only one thread at a time", "A type of CPU", "None"], a: 1 },
        { q: "In MPI, which rank receives data in 'MPI_Scatter'?", o: ["Only rank 0", "All ranks in the communicator", "Only even ranks", "None"], a: 1 },
        { q: "What is 'Latency' in a message passing system?", o: ["Cost of cable", "Time required to initiate a message transfer", "Total bandwidth", "None"], a: 1 },
        { q: "What is the 'Top' function in Linux used for?", o: ["Parallel computing", "Monitoring real-time process activity", "Scheduling jobs", "None"], a: 1 },
        { q: "In CUDA, what is 'Register'?", o: ["A place to sign in", "The fastest memory available to an individual GPU thread", "A type of cable", "None"], a: 1 },
        { q: "Which OpenMP clause specifies variables should be private but initialized with host values?", o: ["private", "firstprivate", "lastprivate", "shared"], a: 1 },
        { q: "What is 'Barrier Synchronization' overhead?", o: ["The cost of a fence", "Time lost waiting for the slowest thread to reach the barrier", "Speed of the network", "None"], a: 1 },
        { q: "Which MPI function is used for point-to-point communication?", o: ["MPI_Bcast", "MPI_Send", "MPI_Reduce", "MPI_Gather"], a: 1 },
        { q: "What is 'Over-subscription' in parallel computing?", o: ["Buying too many servers", "Running more processes/threads than there are physical cores", "Paying for too much data", "None"], a: 1 },
        { q: "In MPI, what is a 'Derived Datatype'?", o: ["A float", "A user-defined datatype to send non-contiguous data", "A pointer", "None"], a: 1 },
        { q: "What is 'Shared Address Space'?", o: ["A shared office", "Memory model where all processors access the same physical memory", "A shared IP", "None"], a: 1 },
        { q: "What is 'OpenACC'?", o: ["Open Account", "A directive-based programming model for GPU accelerators", "Open Access", "None"], a: 1 },
        { q: "The 'M' in SIMD stands for:", o: ["Multiple", "Main", "Memory", "Management"], a: 0 }
    ],
    "5": [
        { q: "Which MPI function is the most efficient way to sum an array across all processes?", o: ["MPI_Send/Recv loop", "MPI_Reduce", "MPI_Bcast", "MPI_Gather"], a: 1 },
        { q: "In OpenMP, what does '#pragma omp task' allow?", o: ["Deleting tasks", "Asynchronous execution of a block of code", "Closing the program", "None"], a: 1 },
        { q: "What is 'Bank Conflict' in GPU memory?", o: ["Robbing a bank", "Two or more threads in a warp accessing different addresses in the same memory bank", "Slow network", "None"], a: 1 },
        { q: "Which MPI collective function sends a distinct chunk of data to every process from a root?", o: ["MPI_Bcast", "MPI_Scatter", "MPI_Alltoall", "MPI_Gather"], a: 1 },
        { q: "In parallel computing, 'Throughput' is usually measured in:", o: ["Seconds", "Operations per second (e.g., FLOPS)", "Bytes", "None"], a: 1 },
        { q: "What is 'Amdahl's Law' upper limit for speedup if 50% of the code is serial?", o: ["2", "5", "10", "Infinite"], a: 0 },
        { q: "Which OpenMP clause ensures all threads get the value of a variable after the loop finishes?", o: ["private", "firstprivate", "lastprivate", "shared"], a: 2 },
        { q: "What is 'Process Rank'?", o: ["The process salary", "A unique integer ID assigned to each process in an MPI communicator", "The CPU priority", "None"], a: 1 },
        { q: "Which CUDA qualifier is used for functions called from and executed on the device?", o: ["__global__", "__device__", "__host__", "None"], a: 1 },
        { q: "What is 'Synchronization' in parallel programming?", o: ["Making clocks the same", "Coordinating the execution of threads/processes to ensure correct results", "A data backup", "None"], a: 1 },
        { q: "What is 'Message Logging'?", o: ["Printing messages", "A fault-tolerance technique to record messages to replay after a crash", "Writing to a log file", "None"], a: 1 },
        { q: "Which MPI function is used to check if a non-blocking operation has finished without waiting?", o: ["MPI_Wait", "MPI_Test", "MPI_Check", "MPI_Query"], a: 1 },
        { q: "What is 'NUMA aware' programming?", o: ["Not using memory", "Designing code to account for non-uniform memory access latencies", "A fast algorithm", "None"], a: 1 },
        { q: "In OpenMP, 'OMP_PROC_BIND' is used for:", o: ["Binding books", "Controlling thread affinity (pinning threads to cores)", "Network security", "None"], a: 1 },
        { q: "What is 'Collective' operation overhead?", o: ["Cost of the cable", "The time it takes to synchronize and communicate across all processes", "Electricity cost", "None"], a: 1 },
        { q: "In CUDA, what is a 'Grid'?", o: ["A graph paper", "The set of all thread blocks for a kernel execution", "A type of GPU", "None"], a: 1 },
        { q: "Which MPI function returns the version of the MPI standard being used?", o: ["MPI_Get_version", "MPI_Version", "MPI_Info", "None"], a: 0 },
        { q: "What is 'Embarrassingly Parallel'?", o: ["Parallel code that is bad", "A task that can be easily split into independent parts with no communication", "A computer error", "None"], a: 1 },
        { q: "What is 'Work-sharing' in OpenMP?", o: ["Sharing a desk", "Distributing the execution of a code region among members of a thread team", "Delegating tasks to users", "None"], a: 1 },
        { q: "Which MPI function allows every process to get the gathered data from everyone?", o: ["MPI_Gather", "MPI_Allgather", "MPI_Scatter", "MPI_Bcast"], a: 1 },
        { q: "What is 'Bit-parallelism'?", o: ["Many CPUs", "Processing multiple bits of data in a single clock cycle", "Network speed", "None"], a: 1 },
        { q: "What is 'Data Race'?", o: ["A race for data", "Concurrent access to a memory location by multiple threads where one is a write", "A fast download", "None"], a: 1 },
        { q: "Which CUDA memory is read-only and optimized for 2D spatial locality?", o: ["Global", "Shared", "Texture", "Local"], a: 2 },
        { q: "In MPI, 'MPI_REDUCE' with 'MPI_SUM' does what?", o: ["Adds all values across processes", "Multiplies all values", "Finds the average", "None"], a: 0 },
        { q: "What is 'SIMD' hardware unit in a CPU often called?", o: ["Arithmetic Unit", "Vector Engine / AVX Unit", "Control Unit", "Cache"], a: 1 },
        { q: "What is 'Loop Unrolling'?", o: ["Deleting a loop", "Expanding a loop to reduce overhead and increase parallelism", "A bug", "None"], a: 1 },
        { q: "Which MPI function is used to free an allocated communicator?", o: ["MPI_Comm_free", "MPI_Comm_delete", "MPI_Finalize", "None"], a: 0 },
        { q: "What is 'Partitioned Global Address Space' (PGAS)?", o: ["A type of RAM", "A memory model combining shared and distributed memory concepts", "A storage method", "None"], a: 1 },
        { q: "In OpenMP, what is 'Section'?", o: ["A part of a book", "A directive for task-level parallelism where different threads run different blocks", "A network area", "None"], a: 1 },
        { q: "Which tool can visualize the execution trace of an MPI program?", o: ["Vampir", "Vim", "Nmap", "Wireshark"], a: 0 },
        { q: "What is 'Scalability'?", o: ["Size of a server", "Ability of an application to use more processors effectively", "Speed of the fan", "None"], a: 1 },
        { q: "What is 'Weak Scaling' used to test?", o: ["If code runs on 1 core", "If the code can solve larger problems with more resources in constant time", "If the network is slow", "None"], a: 1 },
        { q: "In CUDA, what is the 'Register Pressure'?", o: ["Physical pressure", "A condition where a kernel uses too many registers, limiting active warps", "A cooling issue", "None"], a: 1 },
        { q: "Which MPI function performs a prefix sum (cumulative sum) across processes?", o: ["MPI_Reduce", "MPI_Scan", "MPI_Sum", "MPI_Gather"], a: 1 },
        { q: "What is 'Double Buffering'?", o: ["Two monitors", "Overlapping computation with communication using two sets of buffers", "Buying two CPUs", "None"], a: 1 },
        { q: "What is 'CUDA Core'?", o: ["The whole GPU", "The basic processing unit for floating-point/integer operations on an NVIDIA GPU", "The CPU", "None"], a: 1 },
        { q: "Which OpenMP clause specifies the number of threads for a parallel region?", o: ["threads(4)", "num_threads(4)", "nthreads(4)", "None"], a: 1 },
        { q: "What is 'Checkpointing' in HPC?", o: ["Security check", "Periodically saving the state of a long-running parallel job for recovery", "Log files", "None"], a: 1 },
        { q: "In MPI, 'MPI_SENDRECV' is used to avoid what?", o: ["Slow speed", "Deadlock in point-to-point communication", "Network errors", "None"], a: 1 },
        { q: "Parallel computing is essential for:", o: ["Writing emails", "Scientific simulations and Big Data analysis", "Watching movies", "None"], a: 1 }
    ]
};

// --- CORE LOGIC ---
const urlParams = new URLSearchParams(window.location.search);
const setId = urlParams.get('set') || "1";
document.querySelectorAll('.set-btn').forEach(b => b.classList.remove('active'));
if(document.getElementById(`btn-${setId}`)) document.getElementById(`btn-${setId}`).classList.add('active');

const area = document.getElementById('questions-area');
const activeQs = quizData[setId] || quizData["1"];

activeQs.forEach((item, idx) => {
    let html = `<div class="question" id="q-block-${idx}"><span class="q-text">${idx+1}. ${item.q}</span>`;
    item.o.forEach((opt, i) => {
        html += `<label class="option"><input type="radio" name="q${idx}" value="${i}" onclick="clearMark(${idx})"> ${opt}</label>`;
    });
    html += `</div>`;
    area.innerHTML += html;
});

function clearMark(idx) { document.getElementById(`q-block-${idx}`).classList.remove('unanswered'); }

function validateQuiz() {
    let unans = 0;
    activeQs.forEach((_, i) => {
        if(!document.querySelector(`input[name="q${i}"]:checked`)) {
            document.getElementById(`q-block-${i}`).classList.add('unanswered');
            unans++;
        }
    });
    if(unans > 0) alert(`Please answer all questions. ${unans} remaining.`);
    else calculate();
}

function calculate() {
    let score = 0, correct = 0;
    activeQs.forEach((item, i) => {
        const val = document.querySelector(`input[name="q${i}"]:checked`).value;
        if(parseInt(val) === item.a) { score += 3; correct++; } else { score -= 1; }
    });
    const perc = ((correct / activeQs.length)*100).toFixed(1);
    document.getElementById('score-display').innerText = perc + "%";
    document.getElementById('stats-summary').innerHTML = `<p>Correct: ${correct} / ${activeQs.length}</p><p>Score: ${score} points</p>`;
    document.getElementById('result-modal').style.display = 'flex';
}

function closeModal() { document.getElementById('result-modal').style.display = 'none'; window.scrollTo(0,0); }
</script>
</body>
</html>